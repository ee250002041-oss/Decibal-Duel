import os
import glob
import random
from pathlib import Path
from typing import List

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchaudio
from torchaudio.transforms import MelSpectrogram, AmplitudeToDB, GriffinLim
import numpy as np
import matplotlib.pyplot as plt


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

DATA_ROOT = "drive/MyDrive/organized_dataset/train" 
SAMPLE_RATE = 16000
N_MELS = 128
N_FFT = 1024
HOP_LENGTH = 256
TARGET_FRAMES = 512
BATCH_SIZE = 16
LATENT_DIM = 100
EMBED_DIM = 32
NUM_EPOCHS = 200
LR = 2e-4
BETA1 = 0.5
SAVE_DIR = "gan_outputs"
SAVED_WAV_SR = SAMPLE_RATE
SAVE_EVERY = 5 

os.makedirs(SAVE_DIR, exist_ok=True)

class AudioFolderDataset(Dataset):
    def __init__(self, root_dir: str, sample_rate=SAMPLE_RATE, target_frames=TARGET_FRAMES):
        self.root_dir = Path(root_dir)
        self.sample_rate = sample_rate
        self.target_frames = target_frames

        self.classes = sorted([p.name for p in self.root_dir.iterdir() if p.is_dir()])
        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}
        self.files = []
        for cls in self.classes:
            for p in self.root_dir.joinpath(cls).glob("*.wav"):
                self.files.append((str(p), self.class_to_idx[cls]))

        self.mel_spec = MelSpectrogram(
            sample_rate=self.sample_rate,
            n_fft=N_FFT,
            hop_length=HOP_LENGTH,
            n_mels=N_MELS,
        )
        self.to_db = AmplitudeToDB()

    def __len__(self):
        return len(self.files)

    def load_wave(self, path):
        wav, sr = torchaudio.load(path)
        if wav.shape[0] > 1:
            wav = torch.mean(wav, dim=0, keepdim=True)  
        if sr != self.sample_rate:
            wav = torchaudio.functional.resample(wav, sr, self.sample_rate)
        return wav

    def pad_or_truncate_spec(self, spec):
        frames = spec.shape[-1]
        if frames < self.target_frames:
            pad = self.target_frames - frames
            spec = F.pad(spec, (0, pad))
        elif frames > self.target_frames:
            spec = spec[:, : self.target_frames]
        return spec

    def __getitem__(self, idx):
        path, label = self.files[idx]
        wav = self.load_wave(path)
        spec = self.mel_spec(wav)
        spec = spec.squeeze(0)
        spec_db = self.to_db(spec) 
        spec_db = self.pad_or_truncate_spec(spec_db) 
        spec_min, spec_max = spec_db.min(), spec_db.max()
        spec_norm = 2 * (spec_db - spec_min) / (spec_max - spec_min + 1e-9) - 1

        return spec_norm.unsqueeze(0).float(), torch.tensor(label, dtype=torch.long)

class CGAN_Generator(nn.Module):
    def __init__(self, latent_dim, num_classes, embed_dim, ngf=64):
        super().__init__()
        self.num_classes = num_classes
        self.latent_dim = latent_dim
        self.embed = nn.Embedding(num_classes, embed_dim)

        input_dim = latent_dim + embed_dim

        self.init_channels = ngf * 8
        self.init_h = N_MELS // 8 
        self.init_w = TARGET_FRAMES // 8

        self.fc = nn.Linear(input_dim, self.init_channels * self.init_h * self.init_w)

        self.net = nn.Sequential(
            # block 1
            nn.ConvTranspose2d(self.init_channels, ngf * 4, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),

            # block 2
            nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),

            # block 3
            nn.ConvTranspose2d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),

            # final: reduce to 1 channel
            nn.Conv2d(ngf, 1, kernel_size=3, padding=1),
            nn.Tanh()
        )

    def forward(self, z, labels):
        emb = self.embed(labels)   
        x = torch.cat([z, emb], dim=1)
        x = self.fc(x)
        x = x.view(x.size(0), self.init_channels, self.init_h, self.init_w)
        x = self.net(x)
        return F.interpolate(x, size=(N_MELS, TARGET_FRAMES), mode='bilinear', align_corners=False)

class CGAN_Discriminator(nn.Module):
    def __init__(self, num_classes, embed_dim, ndf=64):
        super().__init__()
        self.embed = nn.Embedding(num_classes, embed_dim)

        self.label_to_map = nn.Sequential(
            nn.Linear(embed_dim, N_MELS * TARGET_FRAMES),
        )

        self.net = nn.Sequential(
            nn.Conv2d(2, ndf, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ndf * 4, 1, kernel_size=4),
        )

    def forward(self, spec, labels):
        emb = self.embed(labels)
        label_map = self.label_to_map(emb).view(spec.size(0), 1, N_MELS, TARGET_FRAMES)
        x = torch.cat([spec, label_map], dim=1)
        out = self.net(x)
        out = out.view(out.size(0), -1)
        return out.mean(1)  

def save_spectrogram_image(spec_tensor, path):
    if isinstance(spec_tensor, torch.Tensor):
        spec_np = spec_tensor.cpu().numpy()
    else:
        spec_np = spec_tensor
    plt.figure(figsize=(8, 4))
    plt.imshow(spec_np, origin="lower", aspect="auto")
    plt.colorbar(format="%+2.0f dB")
    plt.tight_layout()
    plt.savefig(path)
    plt.close()

def denormalize_spec(spec_norm, orig_min=-80.0, orig_max=0.0):
    return (spec_norm + 1) / 2 * (orig_max - orig_min) + orig_min

def synth_wave_from_mel_db(mel_db_tensor, sample_rate=SAVED_WAV_SR, n_fft=N_FFT, hop_length=HOP_LENGTH):
    mel_db = mel_db_tensor.unsqueeze(0)
    mel_amp = (10.0 ** (mel_db / 20.0)).cpu()  
    try:
        lin_spec = torchaudio.functional.inverse_mel_scale(mel_amp, n_stft=(n_fft // 2) + 1, sample_rate=sample_rate, n_mels=N_MELS)
    except Exception:
        mel_fb = torchaudio.functional.create_fb_matrix(n_freqs=(n_fft // 2) + 1, n_mels=N_MELS, f_min=0.0, f_max=sample_rate/2.0, norm=None)
        pinv = torch.pinverse(mel_fb)
        lin_spec = torch.matmul(pinv, mel_amp)
        lin_spec = lin_spec.clamp(min=1e-9)

    griffin = GriffinLim(n_fft=n_fft, hop_length=hop_length)
    waveform = griffin(lin_spec.squeeze(0))
    return waveform.unsqueeze(0)

def train():
    dataset = AudioFolderDataset(DATA_ROOT)
    num_classes = len(dataset.classes)
    print(f"Found {len(dataset)} samples across {num_classes} classes: {dataset.classes}")

    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2, pin_memory=True)

    G = CGAN_Generator(LATENT_DIM, num_classes, EMBED_DIM).to(device)
    D = CGAN_Discriminator(num_classes, EMBED_DIM).to(device)

    optG = torch.optim.Adam(G.parameters(), lr=LR, betas=(BETA1, 0.999))
    optD = torch.optim.Adam(D.parameters(), lr=LR, betas=(BETA1, 0.999))
    criterion = nn.BCEWithLogitsLoss()

    fixed_z = torch.randn(num_classes, LATENT_DIM, device=device)
    fixed_labels = torch.arange(num_classes, device=device)

    for epoch in range(1, NUM_EPOCHS + 1):
        for i, (specs, labels) in enumerate(dataloader):
            specs = specs.to(device)
            labels = labels.to(device)
            bsize = specs.size(0)

            D.zero_grad()
            real_logits = D(specs, labels)
            real_targets = torch.ones_like(real_logits, device=device)
            d_real_loss = criterion(real_logits, real_targets)

            z = torch.randn(bsize, LATENT_DIM, device=device)
            rand_labels = torch.randint(0, num_classes, (bsize,), device=device)
            fake_specs = G(z, rand_labels).detach()
            fake_logits = D(fake_specs, rand_labels)
            fake_targets = torch.zeros_like(fake_logits, device=device)
            d_fake_loss = criterion(fake_logits, fake_targets)

            d_loss = (d_real_loss + d_fake_loss) * 0.5
            d_loss.backward()
            optD.step()

            G.zero_grad()
            z = torch.randn(bsize, LATENT_DIM, device=device)
            gen_labels = torch.randint(0, num_classes, (bsize,), device=device)
            gen_specs = G(z, gen_labels)
            gen_logits = D(gen_specs, gen_labels)
            g_targets = torch.ones_like(gen_logits, device=device)
            g_loss = criterion(gen_logits, g_targets)
            g_loss.backward()
            optG.step()

            if i % 50 == 0:
                print(f"Epoch [{epoch}/{NUM_EPOCHS}] Batch [{i}/{len(dataloader)}] D_loss:{d_loss.item():.4f} G_loss:{g_loss.item():.4f}")

        if epoch % SAVE_EVERY == 0 or epoch == 1:
            with torch.no_grad():
                G.eval()
                gen = G(fixed_z, fixed_labels) 
                for idx in range(num_classes):
                    spec_norm = gen[idx, 0]  
                    spec_db = denormalize_spec(spec_norm, orig_min=-80.0, orig_max=0.0)  
                    img_path = os.path.join(SAVE_DIR, f"epoch{epoch}_class{idx}.png")
                    save_spectrogram_image(spec_db.cpu().numpy(), img_path)

                    try:
                        waveform = synth_wave_from_mel_db(spec_db.cpu())
                        wav_path = os.path.join(SAVE_DIR, f"epoch{epoch}_class{idx}.wav")
                        torchaudio.save(wav_path, waveform.cpu(), SAVED_WAV_SR)
                    except Exception as e:
                        print("Warning: could not synthesize waveform:", e)
                G.train()

        torch.save({'G_state': G.state_dict(), 'D_state': D.state_dict(),
                    'optG': optG.state_dict(), 'optD': optD.state_dict()},
                   os.path.join(SAVE_DIR, f"checkpoint_epoch_{epoch}.pt"))

    print("Training finished.")

if __name__ == "__main__":
    train()
